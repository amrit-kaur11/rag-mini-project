# Prompt Engineering & RAG Mini Project 

A **minimal, fully local Retrieval-Augmented Generation (RAG) pipeline** built with  
**FAISS**, **Sentence-Transformers**, and **Ollama** â€” no cloud APIs, no paid keys.


This project was developed as part of an **AI Engineer Intern â€“ Take-Home Assignment**, with a strong focus on:
- Correct retrieval
- Clean prompting
- Reproducibility
- Evaluation


---


## âœ¨ Features


ğŸ”¹ Deterministic text chunking with overlap
ğŸ”¹ Sentence-Transformer embeddings (`all-MiniLM-L6-v2`)
ğŸ”¹ FAISS vector similarity search
ğŸ”¹ Local LLM inference via **Ollama**
ğŸ”¹ Strict JSON-based prompting (reduces hallucinations)
ğŸ”¹ End-to-end CLI demo
ğŸ”¹ Automatic evaluation & CSV results


---


## ğŸ§± Project Structure



rag-mini/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ source.txt # Raw input document
â”‚ â””â”€â”€ cleaned/
â”‚ â””â”€â”€ chunks.jsonl # Chunked text
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ prepare_data.py # Text chunking
â”‚ â””â”€â”€ index_vectors.py # FAISS index creation
â”œâ”€â”€ src/
â”‚ â””â”€â”€ rag.py # Retrieval + generation logic
â”œâ”€â”€ eval/
â”‚ â”œâ”€â”€ evaluate.py # Evaluation script
â”‚ â””â”€â”€ results.csv # Evaluation output
â”œâ”€â”€ query.py # Single-question CLI
â”œâ”€â”€ run_demo.py # Demo questions
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md



---


## âš™ï¸ Setup Instructions


### 1ï¸âƒ£ Create virtual environment


```bash
python -m venv .venv
.venv\Scripts\activate   # Windows
```

2ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```
3ï¸âƒ£ Install & run Ollama

Download from: https://ollama.com

Pull model:

```bash
ollama pull llama3.1:8b
```

---

## ğŸ“„ Data Preparation

Place your raw document inside:

data/source.txt

### Generate chunks:

python scripts/prepare_data.py --input data --output data/cleaned

---

## ğŸ” Build FAISS Index
python scripts/index_vectors.py --chunks data/cleaned/chunks.jsonl --out faiss_index

---

## â“ Ask Questions (CLI)
python query.py --question "What is the refund processing time?"

Example output:

{
  "answer": "14 days after the request is approved",
  "source_chunks": ["source_chunk_0"],
  "answerable": "yes"
}
### â–¶ï¸ Run Demo
python run_demo.py

Outputs are saved to:

demo_outputs.json

---

## ğŸ“Š Evaluation

Run automated evaluation:

python -m eval.evaluate

Results are saved to:

eval/results.csv

---

## ğŸ§  Design Decisions

Local-first: No OpenAI / cloud APIs

FAISS Flat index: Simple & deterministic

JSON-only prompting: Enforces structured outputs

Explicit retrieval â†’ generation separation

Reproducible embeddings & indexing

---

## ğŸš§ Limitations

Single-document ingestion

No reranking stage

No streaming responses

Basic evaluation metrics

---

## ğŸŒ± Future Improvements

Multi-document ingestion

Cross-encoder reranking

Hybrid (BM25 + vector) retrieval

FastAPI backend

Web UI (Streamlit / Next.js)

---

## ğŸ‘©â€ğŸ’» Author

Amrit Kaur
AI / ML Engineer (Internship Candidate)

ğŸ“Œ Notes for Reviewers

This project emphasizes correctness, clarity, and reproducibility over scale.
All components run fully offline using open-source tools.


